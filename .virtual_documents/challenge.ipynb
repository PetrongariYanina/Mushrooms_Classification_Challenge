import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder





df = pd.read_csv('train.csv')
df.head()


df.info()


df.describe()


df['veil-type'].unique()





df = df.drop(columns=['veil-type'])


from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

columns_to_encode = [
    "cap-shape", "cap-surface", "cap-color", "bruises", "odor", "gill-attachment",
    "gill-spacing", "gill-size", "gill-color", "stalk-shape", "stalk-root",
    "stalk-surface-above-ring", "stalk-surface-below-ring", "stalk-color-above-ring",
    "stalk-color-below-ring", "veil-color", "ring-number", "ring-type",
    "spore-print-color", "population", "habitat", "class"
]

for col in columns_to_encode:
    df[col] = label_encoder.fit_transform(df[col])


df.info()


df.head()


from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

X=df.drop(columns=['class'])
y=df['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()

df_train= scaler.fit_transform(X_train)

df_test = scaler.transform(X_test)

X_train.shape, y_train.shape


corr= df.corr()

fig = plt.subplots(figsize=(20,10))
sns.heatmap(corr, annot= True, fmt='.2f')




from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data['df'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range (X.shape[1])]

vif_data


corr_matrix = df[['gill-attachment', 'veil-color', 'ring-number']].corr()
sns.heatmap(corr_matrix, annot=True)
corr_matrix



#df=df.drop(columns=['veil-color', 'Unnamed: 0'])


df





from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier


# K-Nearest Neighbors
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)
print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))

# Árbol de Decisión
decision_tree_model = DecisionTreeClassifier()
decision_tree_model.fit(X_train, y_train)
y_pred_tree = decision_tree_model.predict(X_test)
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_tree))

# Support Vector Machine
svm_model = SVC()
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))

# Bosque Aleatorio
random_forest_model = RandomForestClassifier()
random_forest_model.fit(X_train, y_train)
y_pred_rf = random_forest_model.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))


models = [
  ("K-Nearest Neighbors", KNeighborsClassifier()),
  ("Decision Tree", DecisionTreeClassifier()),
  ("Support Vector Machine", SVC()),
  ("Random Forest", RandomForestClassifier())
]


for name, model in models:
  scores = cross_val_score(model, X, y, cv=5)  
  print(f"{name} Average Accuracy: {scores.mean():.2f} (+/- {scores.std() * 2:.2f})")


from sklearn.model_selection import GridSearchCV

model = RandomForestClassifier(random_state=42)


param_grid = {
    'n_estimators': [10, 50, 100, 200],  
    'max_depth': [None, 10, 20, 30]     
}


grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)


grid_search.fit(X_train, y_train)


print("Best param: ", grid_search.best_params_)


best_model_RF = grid_search.best_estimator_
y_pred_RF = best_model_RF.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred_RF))



model = DecisionTreeClassifier(random_state=42)


param_grid = {
    'max_depth': [None, 10, 20, 30],       
    'min_samples_split': [2, 5, 10],       
    'min_samples_leaf': [1, 2, 4],         
    'criterion': ['gini', 'entropy'],     
}


grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)


grid_search.fit(X_train, y_train)


print("Best param for DecisionTree: ", grid_search.best_params_)


best_model_DT = grid_search.best_estimator_
y_pred_DT = best_model_DT.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred_DT))





from sklearn.preprocessing import OrdinalEncoder

encoder = OrdinalEncoder()

test = pd.read_csv('test.csv')

test = test.drop(columns=['veil-type'])    



test.columns


test.shape, df.shape


columns_to_encode2 = ['Unnamed: 0', 'cap-shape', 'cap-surface', 'cap-color', 'bruises',
       'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',
       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',
       'stalk-surface-below-ring', 'stalk-color-above-ring',
       'stalk-color-below-ring', 'veil-color', 'ring-number', 'ring-type',
       'spore-print-color', 'population', 'habitat']

for col in columns_to_encode2:
    test[col] = label_encoder.fit_transform(test[col])



df_test = scaler.transform(test)

#test=test.drop(columns=['veil-color', 'Unnamed: 0'])


y_pred = best_model_DT.predict(test)


test.shape, y_pred_DT.shape


label_mapping = {0: 'e', 1: 'p'}


y_pred_labels = [label_mapping[label] for label in y_pred]




# prediciendo sobre el test set
print(y_pred_DT[0:20])
# para descargar en ordenador
def download_output(y_pred_labels, name):
  output = pd.DataFrame({'ID': range(1, len(y_pred) +1), 
                         'Edible': y_pred_labels})
  output.to_csv(name, index=False)
download_output(y_pred_labels, 'submission_labels.csv')
